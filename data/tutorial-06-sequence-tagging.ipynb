{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the ktrain [tutorial-06-sequence-tagging](https://github.com/amaiya/ktrain/blob/master/tutorials/tutorial-06-sequence-tagging.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIbBLE_DEVICES\"]=\"0\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ktrain* uses TensorFlow 2.  To support sequence-tagging, *ktrain* also currently uses the CRF module from `keras_contrib`, which is not yet fully compatible with TensorFlow 2.\n",
    "To use the BiLSTM-CRF model (which currently requires `keras_contrib`) for sequence-tagging in *ktrain*, you must disable V2 behavior in  TensorFlow 2\n",
    "by adding the following line to the top of your notebook or script **before** importing *ktrain*:\n",
    "```python\n",
    "import os\n",
    "os.environ['DISABLE_V2_BEHAVIOR'] = '1'\n",
    "```\n",
    "Since we are employing a CRF layer in this notebook, we will set this value here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Tagging\n",
    "\n",
    "Sequence tagging (or sequence labeling) involves classifying words or sequences of words as representing some category or concept of interest.  One example of sequence tagging is Named Entity Recognition (NER), where we classify words or sequences of words that identify some entity such as a person, organization, or location.  In this tutorial, we will show how to use *ktrain* to perform sequence tagging in three simple steps.\n",
    "\n",
    "## STEP 1: Load and Preprocess Data\n",
    "\n",
    "The `entities_from_txt` function can be used to load tagged sentences from a text file.  The text file can be in one of two different formats: 1) the [CoNLL2003 format](https://www.aclweb.org/anthology/W03-0419) or 2) the [Groningen Meaning Bank (GMB) format](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus). In both formats, there is one word and its associated tag on each line (where the word and tag are delimited by a space, tab or comma).  Words are ordered as they appear in the sentence.  In the CoNLL2003 format, there is a blank line that delineates sentences.  In the GMB format, there is a third column for Sentence ID that assignes a number to each row indicating the sentence to which the word belongs.  If you are building a sequence tagger for your own use case with the `entities_from_txt` function, the training data should be formatted into one of these two formats. Alternatively, one can use the `entities_from_aray` function which simply expects arrays of the following form:\n",
    "```python\n",
    "x_train = [['Hello', 'world', '!'], ['Hello', 'Barack', 'Obama'], ['I', 'love', 'Chicago']]\n",
    "y_train = [['O', 'O', 'O'], ['O', 'B-PER', 'I-PER'], ['O', 'O', 'B-LOC']]\n",
    "```\n",
    "Note that the tags in this example follow the [IOB2 format](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)).\n",
    "\n",
    "In this notebook, we will be using `entities_from_txt`  and build a sequence tagger using the Groningen Meaning Bank NER dataset available on Kaggle [here](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus). The format essentially looks like this (with fields being delimited by comma):\n",
    "```\n",
    "      SentenceID   Word     Tag    \n",
    "      1            Paul     B-PER\n",
    "      1            Newman   I-PER\n",
    "      1            is       O\n",
    "      1            a        O\n",
    "      1            great    O\n",
    "      1            actor    O\n",
    "      1            .        O\n",
    " ```\n",
    "\n",
    "We will be using the file `ner_dataset.csv` (which conforms to the format above) and will load and preprocess it using the `entities_from_txt` function.  The output is simlar to data-loading functions used in previous tutorials and includes the processed training set, processed validaton set, and an instance of `NERPreprocessor`.  \n",
    "\n",
    "The Kaggle dataset `ner_dataset.csv` the three columns of interest (mentioned above) are labeled 'Sentence #', 'Word', and 'Tag'.  Thus, we specify these in the call to the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions regarding text.entities_from_text\n",
    "\n",
    "Inputs\n",
    "* use_char - If True, data will be preprocessed to use character embeddings in addition to word embeddings\n",
    "  * The version of TF2 that we use has a bug that prevents from using character encodings. Is this helping to improve the learning process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8 (if wrong, set manually)\n",
      "Number of sentences:  2901\n",
      "Number of words in the dataset:  3222\n",
      "Tags: ['I-component', 'O', 'B-component', 'B-null']\n",
      "Number of Labels:  4\n",
      "Longest sentence: 93 words\n"
     ]
    }
   ],
   "source": [
    "DATAFILE = 'c:/Users/kostobog/Documents/skola/projects/2019-msmt-inter-excelence/partners/nlp-corpus/components-for-ner.csv'\n",
    "(trn, val, preproc) = text.entities_from_txt(DATAFILE,\n",
    "                                             sentence_column='Sentence #',\n",
    "                                             word_column='Word',\n",
    "                                             tag_column='Tag', \n",
    "                                             data_format='gmb',\n",
    "                                             use_char=True,\n",
    "                                             val_pct=0.1\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2610"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trn.xshape()\n",
    "trn.nsamples()\n",
    "# val.nsamples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading the dataset above, we specify `use_char=True` to instruct *ktrain* to extract the character vocabulary to be used in a character embedding layer of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2:  Define a Model\n",
    "\n",
    "The `print_sequence_taggers` function shows that, as of this writing, *ktrain* currently supports both Bidirectional LSTM-CRM and Bidirectional LSTM as base models for sequence tagging. Theses base models can be used with different embedding schemes.\n",
    "\n",
    "For instance, the `bilstm-bert` model employs [BERT word embeddings](https://arxiv.org/abs/1810.04805) as features for a Bidirectional LSTM. See [this notebook](https://github.com/amaiya/ktrain/blob/master/examples/text/CoNLL2002_Dutch-BiLSTM.ipynb) for an example of `bilstm-bert`.  In this tutorial, we will use a Bidirectional LSTM model with a CRF layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bilstm: Bidirectional LSTM (https://arxiv.org/abs/1603.01360)\n",
      "bilstm-bert: Bidirectional LSTM w/ BERT embeddings\n",
      "bilstm-crf: Bidirectional LSTM-CRF  (https://arxiv.org/abs/1603.01360)\n",
      "bilstm-elmo: Bidirectional LSTM w/ Elmo embeddings [English only]\n",
      "bilstm-crf-elmo: Bidirectional LSTM-CRF w/ Elmo embeddings [English only]\n"
     ]
    }
   ],
   "source": [
    "text.print_sequence_taggers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions sequence_tagger\n",
    "**Inputs**\n",
    "* bert_layers_to_use - indices of hidden layers to use.  default:[-2] # second-to-last layer\n",
    "                              To use the concatenation of last 4 layers: use [-1, -2, -3, -4]\n",
    "  * How are the layers used? \n",
    "  * How many layers we should use? Is there a method how to choose the number of layers?\n",
    "*  fasttext and bert embeddings set by parameters *wv_path_or_url* and *bert_model*\n",
    "  * These word embeddings combined using concatenation. \n",
    "  * Do you combined word embeddings in your NER pipeline? \n",
    "  * If yes, do you further process concatenated word embeddings to remove redundancy, e.g. using PCA, see [merging-two-word-embedding-models](https://stats.stackexchange.com/questions/315272/merging-two-word-embedding-models), [ConceptNet at SemEval-2017 Task 2: Extending Word Embeddings with\n",
    "Multilingual Relational Knowledge\n",
    "](https://arxiv.org/pdf/1704.03560.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\anaconda3\\envs\\base-clone\\lib\\site-packages\\ktrain\\text\\ner\\models.py:122: UserWarning: Setting use_char=False:  character embeddings cannot be used in TF2 due to open TensorFlow 2 bug (#33148).\n",
      "Add os.environ[\"DISABLE_V2_BEHAVIOR\"] = \"1\" to the top of script if you really want to use it.\n",
      "  warnings.warn('Setting use_char=False:  character embeddings cannot be used in TF2 due to open TensorFlow 2 bug (#33148).\\n' +\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding schemes employed (combined with concatenation):\n",
      "\tword embeddings initialized with fasttext word vectors (cc.en.300.vec)\n",
      "\tBERT embeddings with bert-base-cased\n",
      "\n",
      "pretrained word embeddings will be loaded from:\n",
      "\tc:/Users/kostobog/ktrain_data/cc.en.300.vec\n",
      "loading pretrained word vectors...this may take a few moments...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WV_URL = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz'\n",
    "model = text.sequence_tagger('bilstm-bert', preproc, wv_path_or_url=WV_URL, bert_model='bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, notice that we suppied the `wv_path_or_url` argument. This directs *ktrain* to initialized word embeddings with one of the pretrained fasttext (word2vec) word vector sets from [Facebook's fastttext site](https://fasttext.cc/docs/en/crawl-vectors.html).   When supplied with a valid URL to a `.vec.gz`, the word vectors will be automatically downloaded, extracted, and loaded in STEP 2 (download location is `<home_directory>/ktrain_data`). To disable pretrained word embeddings, set `wv_path_or_url=None` and randomly initialized word embeddings will be employed. Use of pretrained embeddings will typically boost final accuracy. When used in combination with a model that uses an embedding scheme like BERT (e.g., `bilstm-bert`), the different word embeddings are stacked together using concatenation.\n",
    "\n",
    "Finally, we will wrap our selected model and datasets in a `Learner` object to facilitate training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions regarding ktrain.get_learner\n",
    "* batch_size - Can you give us advice on how to properly select the batch_size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Train and Evaluate the Model\n",
    "\n",
    "Here, we will train for a single epoch using an initial learning rate of 0.01 with gradual decay using cosine annealing (via the `cycle_len=1`) parameter and see how well we do. The learning rate of `0.01` is determined with the learning-rate-finder (i.e., `lr_find`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulating training for different learning rates... this may take a few moments...\n",
      "preparing training data ...done.\n",
      "Epoch 1/1024\n",
      "81/81 [==============================] - 454s 6s/step - loss: 0.6274\n",
      "Epoch 2/1024\n",
      "81/81 [==============================] - 438s 5s/step - loss: 0.6203\n",
      "Epoch 3/1024\n",
      "81/81 [==============================] - 442s 5s/step - loss: 0.6024\n",
      "Epoch 4/1024\n",
      "81/81 [==============================] - 444s 5s/step - loss: 0.5653\n",
      "Epoch 5/1024\n",
      "81/81 [==============================] - 447s 6s/step - loss: 0.4872\n",
      "Epoch 6/1024\n",
      "81/81 [==============================] - 446s 5s/step - loss: 0.3587\n",
      "Epoch 7/1024\n",
      "81/81 [==============================] - 443s 5s/step - loss: 0.2225\n",
      "Epoch 8/1024\n",
      "81/81 [==============================] - 445s 5s/step - loss: 0.1556\n",
      "Epoch 9/1024\n",
      "81/81 [==============================] - 461s 6s/step - loss: 0.1244\n",
      "Epoch 10/1024\n",
      "81/81 [==============================] - 441s 5s/step - loss: 0.0980\n",
      "Epoch 11/1024\n",
      "81/81 [==============================] - 433s 5s/step - loss: 0.0633\n",
      "Epoch 12/1024\n",
      "81/81 [==============================] - 423s 5s/step - loss: 0.0322\n",
      "Epoch 13/1024\n",
      "81/81 [==============================] - 433s 5s/step - loss: 0.0215\n",
      "Epoch 14/1024\n",
      "81/81 [==============================] - 418s 5s/step - loss: 0.0193\n",
      "Epoch 15/1024\n",
      "81/81 [==============================] - 416s 5s/step - loss: 0.0169\n",
      "Epoch 16/1024\n",
      "81/81 [==============================] - 414s 5s/step - loss: 0.0175\n",
      "Epoch 17/1024\n",
      "81/81 [==============================] - 413s 5s/step - loss: 0.0203\n",
      "Epoch 18/1024\n",
      "81/81 [==============================] - 415s 5s/step - loss: 0.0478\n",
      "Epoch 19/1024\n",
      "81/81 [==============================] - 154s 2s/step - loss: 0.0854\n",
      "\n",
      "\n",
      "done.\n",
      "Please invoke the Learner.lr_plot() method to visually inspect the loss plot to help identify the maximal learning rate associated with falling loss.\n"
     ]
    }
   ],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAqiklEQVR4nO3deXxU9b3/8dcnkw1ICEvClrALIiBrBFHcWhdcWlTqXi3K1Yv9aXu9t1Vrr21vva3W9t5btS5Fq9ZeW2tFLW5gXRARVAICgiyGPYCQsISdbJ/fHzN6Y0wggZycTOb9fDzmwcw53znznjHOZ873nPP9mrsjIiKJKynsACIiEi4VAhGRBKdCICKS4FQIREQSnAqBiEiCUyEQEUlwyWEHaKjs7Gzv1atX2DFEROLK/PnzS9w9p7Z1cVcIevXqRUFBQdgxRETiipmtq2uduoZERBKcCoGISIILtBCY2TgzW2FmhWZ2ey3rf2hmC2O3JWZWaWYdgswkIiJfFlghMLMI8CBwLjAQuMLMBlZv4+6/dvdh7j4M+BHwjrtvDyqTiIh8VZB7BKOAQndf7e5lwDPA+EO0vwL4S4B5RESkFkEWglxgQ7XHRbFlX2FmrYFxwNQA84iISC2CLARWy7K6xrz+BvBeXd1CZnaDmRWYWUFxcfERByqrqKJg7XY09LaIyP8JshAUAd2rPc4DNtXR9nIO0S3k7lPcPd/d83Nyar0e4rCKdx/k23/4gG89Mpd/fXYRZRVVR7QdEZGWJshCMA/oZ2a9zSyV6Jf9tJqNzCwLOA34e4BZ+HDNdhZt2Mng3La88NFGbnl2ofYMREQI8Mpid68ws5uAGUAEeNzdl5rZ5Nj6R2JNLwJed/e9QWUBOH9IV0b0bEfXrFb87q1P+c3rKzn/+K6cd3zXIF9WRKTZs3j7VZyfn+9HO8RERWUVFzwwm537yvn7TSfTuW16I6UTEWmezGy+u+fXti4hryxOjiTxm0uGsvtAOROfmMfuA+VhRxIRCU1CFgKAwblZPPTtkazcsptrn5hH8e6DYUcSEQlFwhYCgNP653Df5cP4eGMp33hgNgs37Aw7kohIk0voQgBwwZBuTL3xJJIjxqWPzOWv89aHHUlEpEklfCGAaDfRSzeNZXSfDtw29WN+Nm0plVXxdRBdRORIqRDEtG+TypPXjmLS2N48OWctN/15AQfKK8OOJSISOBWCaiJJxp0XDOTOCwYyfelnfPuxD9i5r+yL9XNWlXDrc4t4+oN1uhhNRFqMuJuqsilMGtubLm3TueWvC5nw8BweumokP5u2lLmrt2EGzxYUsbZkL3ecdxxmtQ2pJCISP1QI6nD+kK5kZ6Ry/VMFnPPbWQDccmZ/bji1D3e/toxH311DekqEfz2rv4qBiMQ1dQ0dwug+HZl640nktmvFVaN78P0z+9EqNcLPvjGIS/PzeOCtQn46bam6iUQkrmmP4DD6dc5k9m1nUP27PinJuOfiIWS1SuHRd9cA8NNvDCKSpD0DEYk/KgT1YGbU7P1JSrIvjhFMmbWaTTsPcN/lw2iTpo9UROKLuoaOglm0GPzsGwN5a/kWJjw8h6Id+8KOJSLSICoEjWDiyb154tpRbNy5n4semsPiop1hRxIRqTcVgkZyWv8cpt54EilJxkUPzeGe15azS6OaikgcUCFoRP07Z/Lq90/h4uG5PPLOKk751dvc98anLN1UqiErRKTZSsiJaZrCko2l/PaNlbyxbCsAbdOTue/y4ZwxoFPIyUQkEWlimhAMzs3ise+cwJzbv8ZvLxsGwN2vLaNKewYi0syoEASsW7tWXDg8l/8YP4iVW/Zw3v3v8vQH68KOJSLyBZ303kTGD81l+95ynpyzhh+/sITyiiomntw77FgiIsHuEZjZODNbYWaFZnZ7HW1ON7OFZrbUzN4JMk+YkpKMSWN78/a/nc5ZAzvzHy9/wkuLNtXa1t3ZuusAc1dtY8nGUnbsLdMwFiISmMAOFptZBFgJnAUUAfOAK9z9k2pt2gFzgHHuvt7MOrn71kNtN14OFh/K/rJKrv7DBxSs28Gw7u1o3zqF8kpn294yduwtY9eBcvaVfXkuhNapEXp1bMMDVw6nb05GSMlFJF4d6mBxkF1Do4BCd18dC/EMMB74pFqbK4Hn3X09wOGKQEvRKjXCnyaN5vezVjF31TaK9xwkOSmJblnpDO7WlratUsht14p+nTPYc6CCTaUH2LRzP8/NL+LfX1jCX244Mey3ICItSJCFIBfYUO1xETC6Rpv+QIqZzQQygfvc/akAMzUbrVIj/MuZ/fmXM+v/nPatU/jN6yvZsusAndumBxdORBJKkMcIahuKs2Y/VDIwEjgfOAe408z6f2VDZjeYWYGZFRQXFzd+0jhx3vFdSTK4/81Pw44iIi1IkIWgCOhe7XEeUPPoaBEw3d33unsJMAsYWnND7j7F3fPdPT8nJyewwM1dn5wMrj25N09/sJ5n5204/BNEROohyEIwD+hnZr3NLBW4HJhWo83fgVPMLNnMWhPtOloWYKa4d9u4AYw9Jptbpy7mZ9OWUrLnYNiRRCTOBXaMwN0rzOwmYAYQAR5396VmNjm2/hF3X2Zm04HFQBXwmLsvCSpTS5CanMQfJuZz18uf8NTctTw1dy1D8tpxar9sTumfw7Du7UiJ6DpBEak/jTUUxz7dspuXF2/m3U+LWbhhJ1UO2RlpXDCkKyN7tie/V3u6ZrUKO6aINAOHOn1UhaCFKN1fznuFJTy/oIjZhSUcKK8C4JhOGVw5qgcTRuaR1Sol5JQiEhYVggRTXlnFss27mLd2By8v3sRH63eSnpLEhcNyuXpMTwZ1ywo7oog0MRWCBLdkYylPf7COFz/axMGKSqbdNJbBuSoGIolEw1AnuMG5Wdx98RBm3XoGZsZrSzaHHUlEmhEVggSSk5nGyJ7teWnRZvaVVYQdR0SaCRWCBPO9r/Vjw4593Pni0rCjiEgzoUKQYMb2y+bmr/Vj6oIini3Q1ckiokKQkL7/9X6c1Lcjd764hE827Qo7joiETIUgAUWSjPsuH05WqxSuf6qApZtKw44kIiFSIUhQOZlpPPadfCqqqrjooTn8+YP1YUcSkZCoECSwIXntePV7p3Bin47c8cLHPKDhrUUSkgpBguuYkcYTE0/g4hG5/Nc/VvLorNVhRxKRJhbkDGUSJyJJxr0ThnCwoopfvLqM9JQkrh7TK+xYItJEVAgEgORIEr+9bBhlFVXc+felRJKSuHJ0j7BjiUgTUNeQfCElksTvrhzOGcfmcMcLH/OLVz6hsiq+xqISkYZTIZAvSUuOMOWafK4Z05NH313DpD/Oo3R/edixRCRAKgTyFSmRJH4+fjC/uGgw7xWWMP53s1n+mS48E2mpVAikTleN7skzN5zIvrJKLnzwPZ6dt4F4G7ZcRA5PhUAOaWTPDrzyvVMY0aM9t05dzAUPzOahmYUqCCItiAqBHFZOZhp/mjSaO84bwNJNu7h3+greWLY17Fgi0kgCLQRmNs7MVphZoZndXsv6082s1MwWxm4/CTKPHLlIknHDqX2Z+YPTSU1O4u5Xl1FRWRV2LBFpBIEVAjOLAA8C5wIDgSvMbGAtTd9192Gx28+DyiONo1d2G+6/fBirS/by0uJNYccRkUYQ5B7BKKDQ3Ve7exnwDDA+wNeTJnL2wC4M6JLJA28W6joDkRYgyEKQC1Sf+aQotqymMWa2yMxeM7NBAeaRRpKUZHzv6/1YXbKXGUs/CzuOiBylIAuB1bKs5s/HBUBPdx8KPAC8WOuGzG4wswIzKyguLm7clHJEzhnUhd7Zbfj9O6t0BpFInAuyEBQB3as9zgO+1Kns7rvcfU/s/qtAipll19yQu09x93x3z8/JyQkwstRXJMm47uReLCoqZeGGnWHHEZGjEGQhmAf0M7PeZpYKXA5Mq97AzLqYmcXuj4rl2RZgJmlEF43IIyMtmafmrgs7iogchcAKgbtXADcBM4BlwLPuvtTMJpvZ5FizbwFLzGwRcD9wuaufIW5kpCUzYUQuryzeTMmeg2HHEZEjZPH2vZufn+8FBQVhx5CYwq17OPO/3+GH5xzL/zvjmLDjiEgdzGy+u+fXtk5XFstROaZTBicf05Gn31+nC8xE4pQKgRy1a8b0YlPpAd5crmEnROKRCoEcta8P6ES3rHT+pIPGInFJhUCOWnIkiatO7MnswhJWfLY77Dgi0kAqBNIorhzVg8y0ZP77HyvCjiIiDaRCII2ifZtUrj+1DzOWbuGj9TvCjiMiDaBCII1m0tjeZGek8bOXNOm9SDxRIZBG0yYtmX8//zgWbdjJM/PWhx1HROpJhUAa1fhh3TixTwfunb5CVxuLxAkVAmlUZsZ/XjiYvQcruOe15WHHEZF6UCGQRndMp0yuP7UPz80v4sM128OOIyKHoUIggbj5a8eQ264Vd764hHINPSHSrKkQSCBapybz028MZMWW3Tw+e03YcUTkEFQIJDBnDezMmcd15n/eWMmG7fvCjiMidVAhkMCYGT8fP4gkM37y9yWa0lKkmVIhkEB1a9eKfzv7WN5eUcwrH28OO46I1EKFQAI38aReHJ+bxX+89Aml+8vDjiMiNagQSOAiScbdFx/P9r1l/OfLn4QdR0RqUCGQJjE4N4vJp/Xhb/OLeFsT2Ig0KyoE0mS+9/V+9O+cwe3PL1YXkUgzEmghMLNxZrbCzArN7PZDtDvBzCrN7FtB5pFwpSVH+M0lQynZU8bDM1eFHUdEYgIrBGYWAR4EzgUGAleY2cA62v0KmBFUFmk+huS144Re7XmvsCTsKCISE+QewSig0N1Xu3sZ8AwwvpZ2NwNTAXUcJ4gTenVg6aZS9hysCDuKiBBsIcgFNlR7XBRb9gUzywUuAh4JMIc0M/m9OlDlsHD9zrCjiAjBFgKrZVnNS0t/C9zm7pWH3JDZDWZWYGYFxcXFjZVPQjKiRzuSDD5cq5FJRZqDIAtBEdC92uM8YFONNvnAM2a2FvgW8JCZXVhzQ+4+xd3z3T0/JycnoLjSVDLTUxjULYu5q3ScQKQ5CLIQzAP6mVlvM0sFLgemVW/g7r3dvZe79wKeA77r7i8GmEmaiVP6ZbNg/U52HdBppCJhC6wQuHsFcBPRs4GWAc+6+1Izm2xmk4N6XYkPp/bPobLKmbtqW9hRRBJecpAbd/dXgVdrLKv1wLC7TwwyizQvI3q0p01qhFkrizlnUJew44gkNF1ZLKFITU5iTN9sZn1arOGpRUKmQiChOa1/Nhu272ftNk1aIxKmehUCM/u+mbW1qD+Y2QIzOzvocNKyndo/egbYOyt0LaFImOq7R3Cdu+8CzgZygGuBewJLJQmhZ8c29Mlpw5sajVQkVPUtBJ9fHHYe8IS7L6L2C8ZEGuSsgZ2Zu2qbRiMVCVF9C8F8M3udaCGYYWaZQFVwsSRRnD2wCxVVzkx1D4mEpr6FYBJwO3CCu+8DUoh2D4kcleHd25Gdkcbrn2wJO4pIwqpvIRgDrHD3nWb2beDfgdLgYkmiSEoyzhrYiZnLt3Kw4pBDTolIQOpbCB4G9pnZUOBWYB3wVGCpJKGMG9yVvWWVvL1cAwqKhKG+haDCo1f9jAfuc/f7gMzgYkkiOblvR3Iy03h+QVHYUUQSUn0LwW4z+xFwNfBKbFaxlOBiSSJJjiQxfmg33l6xlR17y8KOI5Jw6lsILgMOEr2e4DOiE8z8OrBUknAuHpFHeaXzwkcbw44iknDqVQhiX/5PA1lmdgFwwN11jEAazcBubRneox1/en8dVVUae0ikKdV3iIlLgQ+BS4BLgQ/M7FtBBpPEM/GkXqwp2cusT3XQWKQp1bdr6MdEryH4jrtfQ3Ri+juDiyWJ6NzBXcnJTOOpuevCjiKSUOpbCJLcvfqln9sa8FyReklNTuLKUT14e8VW1m3bG3YckYRR3y/z6WY2w8wmmtlE4BVqTDgj0hiuGt2DiJn2CkSaUH0PFv8QmAIMAYYCU9z9tiCDSWLq1Dad847vyrPzNmg+Y5EmUu/uHXef6u7/6u63uPsLQYaSxHbDqX3YfbCC/31fewUiTeGQhcDMdpvZrlpuu81sV1OFlMQyODeLU/vn8PjsNRwo1/hDIkE7ZCFw90x3b1vLLdPd2x5u42Y2zsxWmFmhmd1ey/rxZrbYzBaaWYGZjT2aNyMtx42n9aVkTxl/K9gQdhSRFi+wM39iw1A8CJwLDASuMLOBNZq9CQx192HAdcBjQeWR+HJinw6M6NGOh2au0l6BSMCCPAV0FFDo7qvdvQx4huigdV9w9z2xwewA2gC6pFQAMDN+eM4ANpce4In31oYdR6RFC7IQ5ALV9+uLYsu+xMwuMrPlRE9JvS7APBJnxvTtyNcHdOKhmYUajE4kQEEWgtrmNP7KL353f8HdBwAXAnfVuiGzG2LHEAqKizX8QCK57dwB7D1YwX//Y2XYUURarCALQRHQvdrjPGBTXY3dfRbQ18yya1k3xd3z3T0/Jyen8ZNKs9W/cybXjOnF0x+sY8lGTYonEoQgC8E8oJ+Z9TazVOByYFr1BmZ2jJlZ7P4IIJXo8BUiX7jlrP50aJPKnX9fopFJRQIQWCFw9wrgJmAGsAx41t2XmtlkM5scazYBWGJmC4meYXRZtYPHIgBktUrhR+cex0frd/LnD9eHHUekxbF4+97Nz8/3goKCsGNIE3N3rnn8Qxas28H0fzmV7h1ahx1JJK6Y2Xx3z69tnUYQlbhgZtwzYQhmxm1TF6uLSKQRqRBI3Mht14ofn38cc1Zt4+kPNA6RSGNRIZC4cvkJ3TmlXza/fHW55iwQaSQqBBJXzIx7vzWESncefXd12HFEWgQVAok7XbNaccGQrrywYCN7DlaEHUck7qkQSFy6anRP9pZV8veFG8OOIhL3VAgkLo3o0Y7jurblf99fT7ydAi3S3KgQSFwyM64a3YNlm3exYP3OsOOIxDUVAolbFw7PpW16Mg/PXBV2FJG4pkIgcSsjLZnrT+nDG8u2sLhoZ9hxROKWCoHEtYkn96Jd6xR+87qGqRY5UioEEtcy01P47ul9mbWymDmFJWHHEYlLKgQS964Z04tuWencM325ziASOQIqBBL30lMi3HJWfxYXlfLqx5+FHUck7qgQSItw8Yg8ju2cyd2vLWN/WWXYcUTiigqBtAiRJOPn4wdRtGM/97/1adhxROKKCoG0GKP7dOSSkXk8Oms1yz/bFXYckbihQiAtyh3nHUfbVinc8fzHmrxGpJ5UCKRFad8mlR+fdxwL1u/kL/M0v7FIfagQSItz8YhcxvTpyD2vLWfLrgNhxxFp9lQIpMUxM35x0WAqKp2b//IRFZVVYUcSadYCLQRmNs7MVphZoZndXsv6q8xscew2x8yGBplHEkefnAx+efFgPlyznXtnrAg7jkizlhzUhs0sAjwInAUUAfPMbJq7f1Kt2RrgNHffYWbnAlOA0UFlksRy0fA85q/bwZRZqzmmUwaX5ncPO5JIsxTkHsEooNDdV7t7GfAMML56A3ef4+47Yg/fB/ICzCMJ6KffGMQp/bK54/mPmf2pxiISqU2QhSAX2FDtcVFsWV0mAa/VtsLMbjCzAjMrKC4ubsSI0tKlRJJ48KoR9M3J4Mb/nc/KLbvDjiTS7ARZCKyWZbWe2G1mZxAtBLfVtt7dp7h7vrvn5+TkNGJESQRt01N4/NoTSE+NMPHxD1lbsjfsSCLNSpCFoAio3imbB2yq2cjMhgCPAePdfVuAeSSB5bZrxZPXnsCBiiq+9cgclmwsDTuSSLMRZCGYB/Qzs95mlgpcDkyr3sDMegDPA1e7u2YWkUAN6pbF3yaPITWSxOVT3uft5VvDjiTSLARWCNy9ArgJmAEsA55196VmNtnMJsea/QToCDxkZgvNrCCoPCIAfXMymPrdk+jRoTWT/jhPxwxEAIu3iTzy8/O9oED1Qo7O9r1lnHTPm5w9sAv3XzE87DgigTOz+e6eX9s6XVksCalDm1SuP6UP0xZt4sM128OOIxIqFQJJWN89/Ri6ZaVzxwsfs/dgRdhxREKjQiAJq1VqhF9fMpTVxXu49bnFmu9YEpYKgSS0k4/J5tZxA3jl4838arrGJJLEFNhYQyLx4p9P7cOG7ft45J1VdMpM47qxvcOOJNKkVAgk4ZkZPx8/mG17yvj5y5/gwCQVA0kg6hoSASJJxn1XDGPcoC7c9fIn3Dt9uY4ZSMJQIRCJSUuO8OBVI7hiVA8emrmKHz63mAPllWHHEgmcuoZEqokkGb+8aDCdMtO4781PWfHZbh7+9gjy2rcOO5pIYLRHIFKDmXHLWf2ZcvVI1pbs5YIHZvPax5vVVSQtlgqBSB3OHtSFaTePpVtWK258egGXTXmft1dsVUGQFkdjDYkcRkVlFX/+cD0Pz1zF5tID5LVvxfnHd+W0Y3NYVbyXscdk0zu7TdgxRQ7pUGMNqRCI1FNZRRWvfryZFxduZPanJVRURf/fSU1O4q7xg7jshB4hJxSp26EKgQ4Wi9RTanISFw7P5cLhuezcV8brS7dQvOcg76/exm1TP2bZ5t3cclZ/slqlhB1VpEG0RyBylCoqq/jPV5bx5Jy1tE1P5uoxPbn25N5kZ6SFHU3kC+oaEmkCSzaW8uDbhUxf+hkpkSTGD+3Gd07qxeDcrLCjiagQiDSlVcV7eHz2Gp5fsJH95ZWM6NGO75zUi3GDu5CWHAk7niQoFQKREJTuL2fq/CL+9P461pTspV3rFMYP7cYl+d0Z1K0tZhZ2REkgKgQiIaqqcmYXlvBswQZe/2QLZRVVDOiSySX53ZkwIpd2rVPDjigJQIVApJko3VfOtEUb+dv8IhYXlZKWnMRlJ3TnjvOOIz1F3UYSnNDmLDazcWa2wswKzez2WtYPMLO5ZnbQzH4QZBaR5iCrdQpXj+nFtJvG8tr3T+HiEbk8NXcdl/1+LkU79oUdTxJUYIXAzCLAg8C5wEDgCjMbWKPZduB7wG+CyiHSXB3XtS13XzyE3189ktXFezn//tm8s7I47FiSgILcIxgFFLr7ancvA54Bxldv4O5b3X0eUB5gDpFm7ZxBXXj5e2PpmpXOxCc+5N7pyzX8tTSpIAtBLrCh2uOi2LIGM7MbzKzAzAqKi/WLSVqenh3b8Px3T+KSkXk8NHMVFzwwm/cKS8KOJQkiyEJQ27lxR3Rk2t2nuHu+u+fn5OQcZSyR5ql1ajL3fmsoT157AvvLKrnqsQ+47sl5FG7dHXY0aeGCLARFQPdqj/OATQG+nkiLcPqxnXjz307j9nMHMG/Nds757bv84G+LWLZ5V9jRpIUKctC5eUA/M+sNbAQuB64M8PVEWoz0lAiTT+vLpfndeeCtT3nmww08N7+Ik/p2ZNLY3pxxbCeSknRBmjSOQK8jMLPzgN8CEeBxd/+FmU0GcPdHzKwLUAC0BaqAPcBAd6/zp4+uI5BEVLqvnL/MW88f56xlc+kBeme34dqTezFhRB5t0jSIsByeLigTaSHKK6t4bcln/GH2GhZt2Enb9GSuGNWDG0/vqyuU5ZA0H4FIC5ESSeKbQ7vxzaHdmL9uB4/PXsOj765mwfod/Pn6E0mJaPZZaTj91YjEqZE92/PgVSP4n8uGMW/tDm7560IqKqvCjiVxSHsEInFu/LBcPis9wN2vLcfM+J9Lh5KsPQNpABUCkRbgn0/rS5XDr6Yv52B5Jf9z2TAdRJZ6088GkRbixtP78pMLBvLGsi1MeHgOn27RhWhSPyoEIi3IdWN788S1o9iy6wDn3f8u905fTuk+DeUlh6ZCINLCnNY/h3/862lcMKQbD81cxdh73+Le6cvZtHN/2NGkmdJ1BCIt2LLNu/jtGyt5/ZMtJJlx9sDOXD2mJyf27qgrk+PMQzMLOfO4zvTvnHlEz9d1BCIJ6riubfn91fls2L6P/31/Hc/M28BrSz4jr30rJozIY8KIPHp0bB12TDmMP8xew73TV1C6v5wfnXtco29fewQiCWR/WSUzln7Gc/OLeG9VCe4wqncHLhqey4QReaQmq7e4uXlp0SZu/stHjBvUhQevGkHkCPfkNMSEiHzFpp37eeGjjUydX8Tqkr3k92zPf186THsIzcislcX80x8LGNo9iz9NGn1U81qrEIhIndydlxZv5vapi6modC4Y2pWrRvdgePf2Oo4Qog9Wb+Oaxz+kd3YbnrnhxKMeS0rHCESkTmbGN4d2Y3TvDvzurUKeX1DE8ws2kpOZxun9cxjVuwMn9OpAz46tMVNhCFp5ZRX/+GQLtz63mO4dWvP0P40OfEBB7RGIyJfsOVjBjCWf8dbyrbz7aTG7DlQAkJ2RxrDu7Tg+N4vj89oyODeLTpnpIadtOVYV7+HZgg1Mnb+Rkj0H6Z3dhj9fP5quWa0aZfvaIxCRestIS2bCyDwmjMyjqsopLN7DvLXbKVi7g0VFO3lz+RY+//3YuW1atDDktmNIXhbH52WRnZEW7huII6X7y3ll8Waem7+BBet3EkkyvjagE5fld+f0Y3OabMwo7RGISIPsOVjB0o2lfLyxlCUbS1m8sZQ1JXu/KA7dO7RiSF47BnZty7GdM+nXOYPObdOP6kBnS7Jl1wHeWr6Vf3yyhdmFJZRVVNGvUwYTRuZx8YjcwPaytEcgIo0mIy2Z0X06MrpPxy+WfV4cFhXtZOGGnSzasJNXFm/+0vPapieTk5lGdkYaGWnJtEqN0Do1QuvU2P2UCJ3apjGgS1v65LQhMz2lqd9ao3F31m3bx4Yd+/is9ACflR5g3fZ9FKzdztpt+4Bowbz6xJ6MH9aN43OzQj3+okIgIkettuKw+0A5K7fsZtXWvWzdfYCtuw9SvPsgJXsOsrn0AAfKK9lXVsm+sgr2l1dSXvnl3omsVinktW9FXvtWdM1qFSsaEdJTIqREkmI3++J+csRIjd1vkxahc9t0cjLTjmqynsoqZ395LGNZJWUVVZRVVrG/rJKSPWWU7Im+n23V7pfsKaN490H2HKz40rayM1IZ3qM93z6xJycfk82ALpnN5uC7CoGIBCIzPYWRPTswsmeHerUvq6hic+l+lm3ezbpteynasZ+iHftYVbyXOau2sb+skoqqhnVlm0G7Vim0b51Ku9bRf1unJVPlTlWVUxm7fV6Qov9Wsr+8kr0HKzhYUb+Jftq1TiE7I43sjFQGdWtLdkYa/TtnckynDLq0TadT27Rm3TWmQiAizUJqchI9O7ahZ8c2dbYpq6jiQEUlFZVOeWUVZRVVVFT93/3yyujjXfvL2bLrIJ/tOsCOvWVs31fGzn1lbC49wL6yCpKSjIgZkaTorXVqhKzWqXRrF/lyl1VKhDZpEVqlJtM6JUJaSnSPIz0lQsc2qeRkptGhTWrcTxEaaCEws3HAfUAEeMzd76mx3mLrzwP2ARPdfUGQmUQkfqUmJ2kYjAAE9omaWQR4EDgXGAhcYWYDazQ7F+gXu90APBxUHhERqV2QpXUUUOjuq929DHgGGF+jzXjgKY96H2hnZl0DzCQiIjUEWQhygQ3VHhfFljW0DWZ2g5kVmFlBcXFxowcVEUlkQRaC2s6LqnnIvz5tcPcp7p7v7vk5OTmNEk5ERKKCLARFQPdqj/OATUfQRkREAhRkIZgH9DOz3maWClwOTKvRZhpwjUWdCJS6++aaGxIRkeAEdvqou1eY2U3ADKKnjz7u7kvNbHJs/SPAq0RPHS0kevrotUHlERGR2gV6HYG7v0r0y776skeq3Xfg/wWZQUREDi3uRh81s2JgXexhFlB6iPuf/5sNlBzhS1bfbkPW17a85jLlPzzlP3ze6o9ra6P8DV/fHPIfLvuh2tS2vJ+7Z9W6FXeP2xsw5VD3q/1b0Biv0ZD1tS2vuUz5lf9I89f1Xup4H8ofh/kPl/1o8te8xfu12i8d5n71ZY3xGg1ZX9vymsuU//CUv/Zldb2XQ7U5Espf+7KmyF+f5x/N3/8X4q5r6EiYWYHXMSFDPFD+cCl/uJQ/ePG+R1BfU8IOcJSUP1zKHy7lD1hC7BGIiEjdEmWPQERE6qBCICKS4FQIREQSXMIXAjM7xcweMbPHzGxO2HkaysySzOwXZvaAmX0n7DwNZWanm9m7sf8Gp4ed50iYWRszm29mF4SdpaHM7LjYZ/+cmd0Ydp6GMrMLzexRM/u7mZ0ddp6GMrM+ZvYHM3suzBxxXQjM7HEz22pmS2osH2dmK8ys0MxuP9Q23P1dd58MvAz8Mci8NTVGfqKT++QC5URHc20yjZTfgT1AOvGZH+A24NlgUtatkf7+l8X+/i8FmvQUx0bK/6K7Xw9MBC4LMO5XNFL+1e4+Kdik9XCkV+w1hxtwKjACWFJtWQRYBfQBUoFFRKfKPJ7ol331W6dqz3sWaBtv+YHbgX+OPfe5OMyfFHteZ+DpOMx/JtGRdScCF8Rb/thzvgnMAa6Mx/yx5/0XMCKO8zfp/7s1b4EOOhc0d59lZr1qLP5iikwAM3sGGO/udwO17rqbWQ+iQ2DvCjJvTY2R38yKgLLYw8oA435FY33+MTuAtECC1qGRPv8zgDZE/2ffb2avuntVsMmjGuvzd/dpwDQzewX4c4CRa75uY3z+BtwDvObuCwKO/CWN/PcfqrguBHWobfrL0Yd5ziTgicASNUxD8z8PPGBmpwCzggxWTw3Kb2YXA+cA7YDfBZqsfhqU391/DGBmE4GSpioCh9DQz/904GKiRfjVuto1oYb+/d9MdK8sy8yO8WqjG4ekoZ9/R+AXwHAz+1GsYDS5llgI6jX95ZdWuv80oCxHokH53X0f0ULWXDQ0//NEi1lz0eC/HwB3f7LxoxyRhn7+M4GZQYU5Ag3Nfz9wf3BxGqyh+bcBk4OLUz9xfbC4DvE+/aXyh0v5w6X8IWiJhaA+U2Q2Z8ofLuUPl/KHIcwj1Y1w1P4vwGb+79TJSbHl5wEriR69/3HYOZU//KzK3/xuyt98bhp0TkQkwbXEriEREWkAFQIRkQSnQiAikuBUCEREEpwKgYhIglMhEBFJcCoEEjgz29MErzHZzK4J+nVqvOaFZjbwCJ/3k9j9n5nZDxo/XcPF5oZ4+TBtjjezJ5sokjSRljjWkLRQZhZx91pHWPWABhs71GsCFxIdTviTBm72VqJDP8cdd//YzPLMrIe7rw87jzQO7RFIkzKzH5rZPDNbbGb/UW35ixad5Wupmd1QbfkeM/u5mX0AjIk9/oWZLTKz982sc6zdF7+szWymmf3KzD40s5WxkVkxs9Zm9mzstf9qZh+Y2VcmYzGztWb2EzObDVxiZtfHMi8ys6mx7ZxE9Mv812a20Mz6xm7TY+/jXTMbUMu2+wMH3b2klnXDYu9psZm9YGbtY8tPiC2ba2a/thoTocTadDWzWbEsS6q953FmtiCW/c3YslFmNsfMPor9e2wt22tj0YlX5sXaja+2+iWiQydIC6FCIE3GolMJ9iM6ZvswYKSZnRpbfZ27jyQ6S9b3YsPzQnSs/yXuPtrdZ8cev+/uQ4kOu319HS+X7O6jgH8BPh9d9rvADncfAtwFjDxE3APuPtbdnwGed/cTYq+5jOhQAnOIjiHzQ3cf5u6rgCnAzbH38QPgoVq2ezJQ17j5TwG3xfJ9XC33E8Bkdx9D3XNOXAnMcPdhwFBgoZnlAI8CE2LZL4m1XQ6c6u7DgZ8Av6xlez8G3nL3E4AziBa8NrF1BcApdeSQOKSuIWlKZ8duH8UeZxAtDLOIfvlfFFvePbZ8G9EvvqnVtlFGtDsGYD5wVh2v9Xy1Nr1i98cC9wG4+xIzW3yIrH+tdn+wmf0n0TkTMoAZNRubWQZwEvA3sy9GIq5top2uQHEtz88C2rn7O7FFf4xtqx2QGSs8EJ04prYJTuYBj5tZCvCiuy+06FwDs9x9DYC7b4+1zQL+aGb9iA6RnFLL9s4Gvlnt+EU60INoIdwKdKvlORKnVAikKRlwt7v//ksLo19YZwJj3H2fmc0k+sUD0V/m1X8Fl/v/DZBVSd1/wwdraVPbWPF12Vvt/pPAhe6+yKIT0JxeS/skYGfsF/mh7Cf6RVxf9crs0dmyTgXOB/5kZr8GdlL7WPh3AW+7+0UWnWFrZh2vO8HdV9SyLp3o+5AWQl1D0pRmANfFfj1jZrlm1onoF+OOWBEYAJwY0OvPJjpJO7GzfY6v5/Mygc2xX9tXVVu+O7YOj05zusbMLolt38xsaC3bWgYcU3Ohu5cCOz7v2weuBt5x9x3AbjP7/DOptW/ezHoCW939UeAPROfSnQucZma9Y206xJpnARtj9yfW8Z5nADdbbPfGzIZXW9cf+MpxColfKgTSZNz9daJdG3PN7GPgOaJfpNOB5FhXzV3A+wFFeAjIib3ObcBioLQez7sT+AD4B9H+9c89A/wwdjC1L9EiMcnMFgFLgfFf2VK0G2z451+wNXyHaF/8YqLHUH4eWz4JmGJmc4n+Uq8t8+lEjwt8BEwA7nP3YuAG4PlYps+7u+4F7jaz94hOtl6bu4h2GS2OHZy+q9q6M4BX6niexCENQy0Jw8wiQIq7H4h9cb8J9Hf3sibOcR/wkru/Uc/2Ge6+J3b/dqCru38/yIyHyJIGvAOMdfeKMDJI49MxAkkkrYG3Y108BtzY1EUg5pccekL2ms43sx8R/f91HXV35zSFHsDtKgIti/YIREQSnI4RiIgkOBUCEZEEp0IgIpLgVAhERBKcCoGISIJTIRARSXD/H+DdYEABIgNqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions regarding learning rate (LR) selection\n",
    "Consider the graph above. Is it ok to select LR of 0.02? Is the loss increasing between 1e-7 and 1e-6 indicatoin to select a lower learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing validation data ...done.\n",
      "82/82 [==============================] - 477s 6s/step - loss: 0.0712 - val_loss: 0.0344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x211cf9e04c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1e-2, 1, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   F1:  72.39\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   component       0.75      0.71      0.73       272\n",
      "        null       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.75      0.70      0.72       276\n",
      "   macro avg       0.37      0.36      0.36       276\n",
      "weighted avg       0.74      0.70      0.72       276\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7238805970149254"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our F1-score is **84.19** after a single pass through the dataset. Not bad for a single epoch of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke `view_top_losses` to see the sentence we got the most wrong. This single sentence about James Brown contains 10 words that are misclassified.  We can see here that our model has trouble with titles of songs. In addition, some of the ground truth labels for this example are sketchy and incomplete, which also makes things difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total incorrect: 5\n",
      "Word            True : (Pred)\n",
      "==============================\n",
      "In             :O     (O)\n",
      "case           :O     (O)\n",
      "of             :O     (O)\n",
      "an             :O     (O)\n",
      "internal       :O     (O)\n",
      "error          :O     (O)\n",
      "during         :O     (O)\n",
      "operation      :O     (O)\n",
      "or             :O     (O)\n",
      "the            :O     (O)\n",
      "loss           :O     (O)\n",
      "of             :O     (O)\n",
      "a              :O     (O)\n",
      "sensor         :B-component (B-component)\n",
      "signal         :O     (O)\n",
      "the            :O     (O)\n",
      "system         :O     (O)\n",
      "automatically  :O     (O)\n",
      "switches       :B-component (O)\n",
      "to             :O     (O)\n",
      "ECU            :B-component (B-component)\n",
      "B              :I-component (O)\n",
      ".              :O     (O)\n",
      "If             :O     (O)\n",
      "the            :O     (O)\n",
      "loss           :O     (O)\n",
      "of             :O     (O)\n",
      "the            :O     (O)\n",
      "sensor         :B-component (B-component)\n",
      "signal         :O     (O)\n",
      "was            :O     (O)\n",
      "the            :O     (O)\n",
      "cause          :O     (O)\n",
      "for            :O     (O)\n",
      "the            :O     (O)\n",
      "error          :O     (O)\n",
      ",              :O     (O)\n",
      "the            :O     (O)\n",
      "system         :O     (O)\n",
      "automatically  :O     (O)\n",
      "switches       :B-component (O)\n",
      "back           :O     (O)\n",
      "to             :O     (O)\n",
      "ECU            :B-component (B-component)\n",
      "A              :I-component (O)\n",
      ".              :O     (O)\n",
      "A              :O     (O)\n",
      "fault          :O     (O)\n",
      "in             :O     (O)\n",
      "one            :O     (O)\n",
      "of             :O     (O)\n",
      "the            :O     (O)\n",
      "ECUs           :O     (O)\n",
      "is             :O     (O)\n",
      "indicated      :O     (O)\n",
      "by             :O     (O)\n",
      "a              :O     (O)\n",
      "caution        :O     (O)\n",
      "message        :O     (O)\n",
      "on             :O     (O)\n",
      "the            :O     (O)\n",
      "PFD            :B-component (B-component)\n",
      "(              :O     (O)\n",
      "L              :O     (O)\n",
      "/              :O     (O)\n",
      "R              :O     (O)\n",
      "ECU            :B-component (B-component)\n",
      "A              :I-component (O)\n",
      "/              :O     (O)\n",
      "B              :O     (O)\n",
      "FAIL           :O     (O)\n",
      ")              :O     (O)\n",
      ".              :O     (O)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.view_top_losses(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions on New Sentences\n",
    "\n",
    "Let's use our model to extract entities from new sentences. We begin by instantating a `Predictor` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ktrain.get_predictor(learner.model, preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'O'),\n",
       " ('rod', 'B-component'),\n",
       " ('end', 'O'),\n",
       " ('bearing', 'I-component'),\n",
       " ('is', 'O'),\n",
       " ('screwed', 'O'),\n",
       " ('into', 'O'),\n",
       " ('a', 'O'),\n",
       " ('steel', 'B-component'),\n",
       " ('push', 'O'),\n",
       " ('rod', 'I-component'),\n",
       " ('and', 'O'),\n",
       " ('locked', 'O'),\n",
       " ('by', 'O'),\n",
       " ('means', 'O'),\n",
       " ('of', 'O'),\n",
       " ('a', 'O'),\n",
       " ('jam', 'B-component'),\n",
       " ('nut', 'I-component'),\n",
       " ('which', 'O'),\n",
       " ('has', 'O'),\n",
       " ('locking', 'O'),\n",
       " ('varnish', 'O'),\n",
       " ('applied', 'O'),\n",
       " ('to', 'O'),\n",
       " ('it', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('A rod end bearing is screwed into a steel push rod and locked by means of a jam nut which has locking varnish applied to it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the predictor for later deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.save('/tmp/mypred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\apps\\anaconda3\\envs\\base-clone\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    }
   ],
   "source": [
    "reloaded_predictor = ktrain.load_predictor('/tmp/mypred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` method also can accept a list of sentences.  And, larger batch sizes can potentially speed predictions when `predictor.predict` is supplied with a list of examples.\n",
    "\n",
    "Both the `load_predictor` and `get_predictor` functions accept an optional `batch_size` argument used for predictions, which is set to 32 by default. The `batch_size` can also be set manually on the `Predictor` instance.  That is, the `batch_size` used for inference and predictions can be increased with either of the following:\n",
    "```python\n",
    "# you can set the batch_size as an argument to load_predictor (or get_predictor)\n",
    "predictor = ktrain.load_predictor('/tmp/mypred', batch_size=128)\n",
    "\n",
    "# you can also set the batch_size used for predictions this way\n",
    "predictor.batch_size = 128\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
